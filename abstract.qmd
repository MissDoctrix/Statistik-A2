---
title: "KI-Systeme zur Erkennung von Misinformation: Was fordern Nutzende?"
author:
  - "Carys Artemis Lamm"
  - "Lara Senger"
  - "Mino Wittemann"
date: today
date-format: "D. MMMM YYYY"
format:
  pdf:
    papersize: a4
    fontsize: 12pt
    number-sections: true
    fig-cap-location: top
    fig-numbering: true
    tbl-numbering: true
    code-fold: true
  html:
    fontsize: 12pt
    number-sections: true
    fig-numbering: true
    tbl-numbering: true
    code-fold: true
lang: de
bibliography: literatur.bib
csl: apa7.csl
---

**GitHub Repository:** <https://github.com/MissDoctrix/Statistik-A2.git>\
**F√ºr die Abgabe aktueller GitHub Hash:** 2b414e8d0478ee4c6328d2f6641749a22a015246

to Do:

-   Diskussion

-   textliche Verkn√ºpfung und Stilanpassung

-   R√ºckmeldung Ergebnisse (dynamische Referenzierung, Scatterplot (?),)

-   Abschlusspr√§sentation

    -   Einleitung, inkl. Einf√ºhrung, Relevanz und Zielsetzung, 3min (done)

    -   Methodik, inkl. Arten der Erhebungen und Auswertungsstrategie, 1,5min

    -   Ergebnisse (qual. + quant.) inkl. Diskussion der Implika onen und Relevanz, 7,5min

    -   Diskussion der Limitationen und m√∂glicher zuk√ºnftiger Arbeiten, 3min

## Einleitung

Misinformationen auf social Media stellen eine wachsende Herausforderung f√ºr Nutzende dar. Im Jahr 2024 nutzten laut der ARD/ZDF-Medienstudie etwa 60‚ÄØ% der deutschen Bev√∂lkerung ab 14 Jahren soziale Netzwerke (@m√ºller2024). Diese Plattformen dienen zunehmend als Informationsquellen, wobei es f√ºr die Nutzende schwierig ist, falsche oder irref√ºhrende Inhalte zu erkennen. Derartige Inhalte k√∂nnen gesellschaftliche Spannungen versch√§rfen und politische sowie wirtschaftliche Prozesse beeinflussen (@gesing_desinformation_2024). Sp√§testens seit dem US-amerikanischen Pr√§sidentschaftswahlkampf im Jahr 2016 gilt die gezielte Verbreitung von Falschinformationen √ºber social Media als strategisches Mittel zur Meinungsbeeinflussung (@rudloff_fake_2023).

In der Forschung finden sich verschiedene Begriffe wie ‚ÄûFake News‚Äú, ‚ÄûDesinformation‚Äú oder ‚ÄûMisinformation‚Äú, die h√§ufig synonym verwendet werden. Im Rahmen dieser Studie verwenden wir den Begriff Misinformation und orientieren uns dabei an @rudloff_fake_2023, den zufolge es sich um frei erfundene, irref√ºhrende oder manipulierte Inhalte handelt.

Die Auseinandersetzung mit Misinformation ist kaum zu vermeiden. Sie verbreiten sich sowohl √ºber private Kommunikationskan√§le wie Messenger-Dienste als auch algorithmisch verst√§rkt durch social Media Plattfordem, auf welchen polarisierende Beitr√§ge besonderes Sichtbarkeit erlangen. Die kritische Bewertung solcher Inhalte gestaltet sich aus verschiedenen Gr√ºnden als schwierig: Menschen neigen dazu, Informationen so zu interpretieren, dass sie ihr bestehendes Weltbild best√§tigen, und sie √ºbersch√§tzen die Wahrscheinlichkeit wahrer Inhalte aufgrund wiederholter Konfrontation (@rudloff_fake_2023). Diese kognitiven Mechanismen wirken sich insbesondere in schnelllebigen, auf kurze Aufmerksamkeitsspannen ausgelegten social Media aus.

Vor diesem Hintergrund setzt aktuelle Forschung im Bereich der algorithmischen Erkennung von Misinformation an. Die Entwicklung der sogenannten K√ºnstlichen Intelligenz (im Folgenden: KI) nimmt hierbei eine zentrale Rolle ein. KI-basierte Modelle wie FakeBERT (@kaliyar_fakebert_2021) oder exBAKE (@jwa_exbake_2019) weisen bereits eine hohe Detektionsgenauigkeit auf. Eine verl√§ssliche Detektion allein reicht jedoch nicht aus, um Nutzende im praktischen Umgang mit Misinformation effektiv zu unterst√ºtzen.

Ebenso ist entscheidend, wie entsprechende Systeme gestaltet sind und in welcher Weise Menschen mit ihnen interagieren. Erste Studien heben hervor, dass insbesondere transparente Bewertungen, nachvollziehbare Begr√ºndungen von Entscheidungen sowie die Einbindung glaubw√ºrdiger Quellen zentrale Anforderungen darstellen (@amri_afcc_2024; @shu_defend_2019; @szczepanski_new_2021). Dennoch beruhen viele dieser Erkenntnisse bislang √ºberwiegend auf theoretischen Annahmen, systematische empirische Untersuchungen mit Fokus auf Nutzende fehlen weitgehend.

Ziel der vorliegenden Studie ist es daher, ausgehend von den Nutzende, konkrete Anforderungen an KI-gest√ºtzte Tools zur Erkennung von Misinformation zu identifizieren und Faktoren zu bestimmen, die eine erfolgreiche Nutzung beg√ºnstigen. Im Zentrum steht dabei die folgende Forschungsfrage: Welche Anforderungen stellen Nutzende an eine KI-Anwendung zur Erkennung von Misinformation auf Social Media?

Zur Beantwortung dieser Frage kombinieren wir qualitative Leitfadeninterviews mit einer quantitativen mixed Design Umfrage. Der Aufsatz ist in folgende Hauptabschnitte gegliedert: Einleitung, Literatur√ºbersicht, Methodik, Ergebnisse, Diskussion und Fazit. Die Einleitung f√ºhrt in das Thema ein und stellt die Relevanz und Zielsetzung dar. Die Literatur√ºbersicht fasst bestehende Studien und Theorien zusammen und identifiziert Forschungsl√ºcken. Im Methodikteil wird das Vorgehen f√ºr die Leitfadeninterviews und die Gestaltung der digitalen Umfrage ausgef√ºhrt. Anschlie√üend werden die Analyseergebnisse der Interviews und der Umfrage im Ergebnisteil erl√§utert. Zum Schluss werden die Erkenntnisse im Diskussionsteil zusammengefasst und eingeordnet. Zudem findet sich im Anhang unser Code of Conduct, die Literatur und der Rekrutierungstext f√ºr die Online-Studie.

## Literatur√ºbersicht

Es stehen zwei Fragen im Fokus der Literaturrecherche: Erstens, was sind Nutzendenanforderungen an k√ºnstliche Intelligenz als Werkzeug zur Unterst√ºtzung bei der Detektion von Misinformation auf Social Media? Zweitens, wie gehen Personen in diesem Kontext mit unterschiedlichen Feedback-Arten um?

Einen ersten Einblick in den wissenschaftlichen Forschungsstand und in die Thematik bieten @szczepanski_new_2021. Ihre Arbeit pr√§sentiert eine Erkl√§rungsmethode, mit der deep learning Fake‚ÄëNews-Modelle nachvollziehbarer werden, indem es die Textstellen hervorhebt, die das Urteil am st√§rksten beeinflussen. Diese Untersuchung bietet dabei einen Ansatz f√ºr die Frage, wie die Ausgabe von KI Werkzeuge gestaltet werden kann, allerdings beinhaltet die Arbeit keine Untersuchung mit verschiedenen Nutzergruppen und ihrem Feedback.

√Ñhnlichen Einblick bietet @reis_explainable_2019, deren Arbeit ebenfalls die Erkl√§rbarkeit von KI Modellen zur Fake-News-Detektion fokussiert. Durch Verwendung unterschiedlichen Daten versucht das Modell eine ganzheitliche Bewertung zu erreichen und diese nachvollziehbar f√ºr Nutzende zu machen, indem das System hervorhebt, welche Merkmale am meisten zu einer Entscheidung betragen. Allerdings liegt auch hier keine direkte Nutzerforschung vor, die Bed√ºrfnisse der Nutzenden wurden aus Literatur extrahiert und analysiert, jedoch nicht nicht durch eine testende Studie √ºberpr√ºft.

Eine andere Arbeit von @shu_defend_2019 evaluiert in einer Nutzendenstudie, wie gut Erkl√§rungen und Kommentare bez√ºglich der Entscheidung einer KI, ob Nachrichteninhalte Misinformationen sind, verstanden werden. Dies sind wichtige Orientierungspunkte f√ºr die Forschungsfragen. Jedoch untersucht die Arbeit nicht weitergehend, wie unterschiedliche Feedbackformen von den Nutzenden bewertet werden und wie sehr ihnen Vertrauen entgegen gebracht wird.

Andere Forschungsarbeiten fokussieren sich auf die Frage, wie Nutzende Misinformation-Detektionstool wahrnehmen. In einer experimentellen Online-Studie von @ribes_trust_2021 wurden verschiedene Layouts und Szenarien entwickelt, umgesetzt und mit 266 Teilnehmern getestet. Die Ergebnisse legen nahe, dass das Layout einen signifikanten Einfluss darauf hat, wie wichtig Nutzende die Quellenangabe eines Mediums einsch√§tzen. Zudem wirkt sich nach der Studie die Menge der bereitgestellten Informationen zur Erkl√§rung der KI negativ auf das Verst√§ndnis der Nutzende aus. Ein zentraler Faktor f√ºr die Akzeptanz KI-basierter Detektionssysteme stellt Vertrauen dar, identifiziert unter anderen eine Studie von @Shin2023518. Diese stellt fest, dass Vertrauen vor allem durch wahrgenommene Kompetenz, Kooperationsf√§higkeit und Autonomie der Anwendung entsteht. Dies unterstreicht sowohl die Bedeutung als auch die Herausforderungen, ein Detektionstool f√ºr Nutzende verst√§ndlich zu gestalten.

Die f√ºnf genannten Forschungsarbeiten bieten einen ersten Ansatz, sich mit dem Themenfeld vertraut zu machen. Jedoch m√ºssen mehr wissenschaftliche Ver√∂ffentlichungen mit dem Schwerpunkt auf Wahrnehmung und Beurteilung unterschiedlicher Feedbacksysteme durch die Nutzenden herangezogen werden, die zudem den Fokus aus Nutzendentests legen und diese Untersuchungen vergleichen. Ebenso sind mehr Arbeiten zu suchen, die empirisch untersuchen, welche Anforderungen durch die Nutzenden gestellt werden und ob sich diese in bestehenden Modellen bereits wiederfinden.

Ankn√ºpfend an die Literatur liegt unser Fokus auf der Analyse konzeptioneller und gestalterischer Anforderungen, die Nutzende an eine KI-Anwendung stellen.

## Methode

### Qualitative Methode

Um zu untersuchen, welche Anforderungen Nutzende an eine KI-Anwendung zur Erkennung von Misinformation auf Social Media stellen, wurde die qualitative Forschungsmethode des leitfadengest√ºtzten Interviews gew√§hlt. Personeninterviews erm√∂glichen es, Themen aus Sicht der Befragten zu identifizieren und diese in weiteren Analyseschritten zu vertiefen. Durch offen formulierte Fragen k√∂nnen individuelle Perspektiven, relevante Erfahrungen und subjektive Einsch√§tzungen eingebracht werden, die bei standardisierten Frageformaten m√∂glicherweise unber√ºcksichtigt bleiben. Anstatt Themen vorzugeben, erlaubt die Interviewmethode, relevante Inhalte direkt aus den Antworten der Teilnehmenden abzuleiten und f√ºr die Bearbeitung der Forschungsfrage nutzbar zu machen.

F√ºr die Stichprobe wurden gezielt junge Erwachsene ausgew√§hlt, die regelm√§√üig social Media nutzen und bereits Erfahrungen mit k√ºnstlicher Intelligenz gesammelt haben. Insbesondere sind ihnen Large Language Models wie ChatGPT bekannt und sie haben diese bereits genutzt. Die Stichprobe weist somit relevantes Vorwissen hinsichtlich der Funktionsweise sowie der Interaktionsm√∂glichkeiten mit KI-Systemen auf. Die Interviews orientierten sich an einem thematischen Leitfaden und umfassten Fragen zu Vorerfahrungen zu KI, Nutzungsverhalten bez√ºglich Social Media sowie Erwartungen an eine KI-Anwendung, die √ºber erkannte Misinformation informiert. Insgesamt wurden zwei leitfadengest√ºtzte Interviews durchgef√ºhrt und analysiert. Die Gespr√§che wurden √ºber eine mobile Applikation lokal auf dem Ger√§t aufgezeichnet. Die Transkripte wurden zun√§chst mithilfe der Software @whispertranscribe2025 erstellt und anschlie√üend von einer zweiten Person korrigiert und formal √ºberarbeitet.

Die Auswertung erfolgte anhand der sechsstufigen thematischen Analyse. Zur Strukturierung wurde im Programm Excel von @microsoft_excel_2024 eine Tabelle angelegt, in der Codes thematisch geordnet, zu Kategorien zusammengef√ºhrt und mit exemplarischen Zitaten aus den Interviews verkn√ºpft wurden. Die Tabelle diente zugleich als Arbeitsgrundlage zur √úberpr√ºfung, Anpassung und finalen Formulierung der Themen und Definitionen. In mehreren Korrekturschleifen wurden diese √ºberarbeitet und zu einem gemeinsamen Analyseergebnis verdichtet.

### Quantitative Methode

Als quantitative Untersuchung wurde eine Online-Befragung durchgef√ºhrt. F√ºr die Rekrutierung der Teilnehmenden wurden insgesamt sechs Einschluss- bzw. Ausschlusskriterien definiert. Zur Sicherstellung der rechtlichen Einwilligung in die Verarbeitung personenbezogener Daten haben wir ein Mindestalter von 18 Jahren festgelegt. Eine obere Altersgrenze haben wir nicht gesetzt, um eine m√∂glichst vielf√§ltige Stichprobe zu erm√∂glichen. Ausgeschlossen wurden Personen, die aktuell an der Veranstaltung Statistik und Methoden der Nutzerforschung teilnehmen, da ihnen durch die Lehrveranstaltung potenziell Vorwissen zum Studiendesign vorliegt, sie somit nicht unvoreingenommen die Befragung bearbeiten k√∂nnen.Die Befragung wurde ausschlie√ülich in deutscher Sprache durchgef√ºhrt. Entsprechend wurden nur deutschsprachige Personen als Teilnehmende gesucht, um Verst√§ndnisschwierigkeiten vermeiden. Dar√ºber hinaus war f√ºr die Teilnahme der Zugriff Tablet, Laptop oder PC mit aktuellem Browser erforderlich, da die Online-Befragung ausschlie√ülich f√ºr Desktop-Oberfl√§chen konzipiert wurde und eine kompatible Anzeigeumgebung ben√∂tigt. Ein weiteres Einschlusskriterium war die zumindest gelegentliche Nutzung von social Media, da die Untersuchung auf Erfahrungen und Einsch√§tzungen im Kontext von social Media Bezug nehmen. Vorerfahrungen mit KI-Systemen haben wir hingegen bewusst nicht vorausgesetzt, da die KI unterst√ºtzte Anwendung kein Vorwissen ben√∂tigt. Dadurch sollte ein bereiteres Bild hinsichtlich der unterschiedlichen Technikaffinit√§t und des Vorwissens erm√∂glicht werden. Die Befragung wurde im Zeitraum vom Samstag, 24. Mai 2025, bis zum Montag, 2. Juni 2025 durchgef√ºhrt. Die Rekrutierung der Teilnehmenden erfolgte sowohl m√ºndlich im pers√∂nlichen Umfeld als auch durch die Verbreitung mittels eines Rekrutierungstextes. Es wurden so viele Teilnehmende wie m√∂glich aus dem n√§heren Bekanntenumfeld gesucht mit einer Mindestgrenze von 15 Teilnehmenden. Die Befragung wurde von den Teilnehmenden eigenst√§ndig online bearbeitet.

Die Studie folgte einem mixed factorial design, mit between-subjects-Faktoren als auch within-subjects-Faktoren. Alle Teilnehmenden f√ºhrten zun√§chst eine Bewertung von Social Media Posts ohne Unterst√ºtzung durch KI durch (within-subjects-Faktor), gefolgt von einer Bewertung mit KI-Unterst√ºtzung. Die Teilnehmenden wurden randomisiert einer von zwei Varianten eines KI-Systems zugewiesen (between-subjects-Faktor). Die eine H√§fte bewertete eine evaluierende, die andere eine empfehlende KI-Variante.

------------------------------------------------------------------------

![Ablauf der quantitativen Studie](images/Untitled.png){#fig-ablauf-studie}

Die Datenerhebung bestand aus insgesamt f√ºnf Abschnitten (wie in @fig-ablauf-studie vollst√§ndig dargestellt): einem Pre-Test, einer Baseline-Phase, einem Post-Baseline-Fragebogen, dem Test der Anwendung und einem abschlei√üenden Post-Test-Fragebogen. Im Pre-Test wurden grundlegende demografische Daten erfasst, darunter das Alter der Teilnehmenden (metrisch), das Geschlecht (nominalskaliert: m√§nnlich, weiblich, divers) sowie der Bildungsstand (ordinalskaliert mit mehreren Bildungsstufen). Zus√§tzlich wurden Angaben zur Technikaffinit√§t (metrisch), zur Nutzung von social Media (teils ordinal und teils nominal) sowie zum Vorwissen √ºber KI (metrisch) erfasst. Im Hauptteil der Studie mussten die Teilnehmenden 20 Social-Media-Beitr√§ge bewerten, die potentiell Falschinformationen enthielten. Die Beitr√§ge waren in zwei Phasen aufgeteilt: In der ersten Phase (Baseline) wurden zehn Beitr√§ge ohne Unterst√ºtzung durch ein KI-System bewertet. In der zweiten Phase (Test der Anwendung) erfolgte die Bewertung von zehn weiteren Beitr√§gen, hier mit Unterst√ºtzung durch eines von zwei KI-Systemen: entweder ein evaluatives System, das eine eigene Einsch√§tzung abgibt, oder ein empfehlendes System, das eine Handlungsempfehlung formuliert. Die Zuordnung der KI-Systeme erfolgte zwischen den Teilnehmenden randomisiert. Zu jedem Beitrag sollten die Teilnehmenden einsch√§tzen, ob es sich um Falschinformationen handelt (ja/nein). Diese Einsch√§tzung wurde bin√§r erfasst (nominalskaliert). Parallel wurde erfasst, unter welcher Systembedingung der jeweilige Beitrag pr√§sentiert wurde: entweder ohne Assistenz (Baseline), mit einer evaluativen Entscheidungshilfe oder mit einer direkten Empfehlung. Nach jeder der beiden Bewertungsphasenwurde ein Post-Test durchgef√ºhrt, in dem die subjektiv empfundene mentale Belastung abgefragt wurde. Daf√ºr kam hier eine Skala zum Einsatz, die sich an der DLR-Workload-Skala orientiert. Diese Skala misst f√ºnf Bereiche der mentalen Beanspruchung: Informationsaufnahme, Wissensabruf, Entscheidungsfindung, motorische Beanspruchung und zeitlicher Druck. Jeder Bereich wurde einzeln auf einer visuellen Analogskala von 0 (sehr stark unterbeansprucht) bis 200 (sehr stark √ºberbeansprucht) bewertet, wobei 100 als optimaler Wert gilt. Die Teilnehmenden konnten somit sehr differenziert angeben, wie belastend sie die Aufgaben empfanden. Die DLR-Skala gilt als √∂konomisch, da sie trotz weniger Fragen eine differenzierte Erfassung erm√∂glicht. Die bin√§re Entscheidung bei den Beitr√§gen (ja/nein) ist angemessen f√ºr unser Ziel, um herauszufinden, wie sicher und richtig Teilnehmende Falschinformationen erkennen. Insgesamt liefert der Studiendesign sowohl objektive Daten (Entscheidung pro Beitrag), als auch subjektive Einsch√§tzungen (√ºber die Belastung). Damit l√§sst sich analysieren, ob und wie die KI-Systeme einen Einfluss auf das Erkennen von Misinformation haben.

Zur Analyse der erhobenen Daten sind zwei zentrale statistische Verfahren geplant: Ein t-Test f√ºr unabh√§ngige Stichproben sowie eine Korrelationsanalyse nach Pearson. Dadurch k√∂nnen sowohl Gruppenunterschiede als auch die Zusammenh√§nge zwischen ausgew√§hlten Variablen untersucht werden. Zun√§chst wird zur Untersuchung von Gruppenunterschieden ein t-Test f√ºr unabh√§ngige Stichproben eingesetzt. Dieser Test pr√ºft, ob sich die mittlere subjektive Belastung (erhoben mit der DLR Workload-Skala nach @dlr119443) in Abh√§ngigkeit von der Art der KI-Unterst√ºtzung (evaluativ vs. empfehlend) signifikant unterscheidet. Die Belastung wird dabei als metrisch skalierte Variable behandelt, da sie auf einer visuellen Analogskala von 0 bis 200 erhoben wurde. Die Gruppenzuordnung (Art der KI) ist nominalskaliert, weshalb sich der t-Test als geeignete Methode zur Pr√ºfung dieser Hypothese anbietet. Das Ziel dieser Analyse ist es herauszufinden, ob eine bestimmte Form der KI-Assistenz kognitiv entlastender wirkt als die andere, was f√ºr die Gestaltung zuk√ºnftiger Systeme relevant sein k√∂nnte. Erg√§nzend dazu wird eine Korrelation nach Pearson durchgef√ºhrt, um m√∂gliche Zusammenh√§nge zwischen der subjektive empfundenen Belastung und dem Vertrauen in das KI-System zu untersuchen. Das Vertrauen wurde im Post-Test mit Hilfe mehreren Aussagen auf einer metrisch interpretierten Likert-Skala erfasst (@Mandsen_Gregor). Da beide Variablen (subjektive Belastung und Vertrauen) als intervallskaliert gelten, ist der Pearson-Korrelationskoeffizient eine geeignete Methode zur Analyse. Diese Analyse soll zeigen, ob beispielsweise eine h√∂here Belastung mit einem niedrigeren Vertrauen in das System einhergeht. F√ºr beide Analysen wird ein Signifikanzniveau von 5% (Œ± = 0.05) festgelegt.

## Ergebnisse

### Qualitative Ergebnisse

Im Rahmen der Interviews konnten mehrere thematische Schwerpunkte identifiziert werden, die von den Befragten als zentrale Anforderungen an eine KI-Anwendung zur Erkennung von Misinformation auf Social Media formuliert wurden.

| Name | Definition | Textstelle |
|:-----------------------|:-----------------------|:-----------------------|
| Barrierefreiheit | Die Barrierefreiheit beschreibt den Anspruch, dass die Gestaltung der Detektion allen Menschen eine gleichberechtigte und selbstst√§ndige Nutzung erm√∂glicht, unabh√§ngig von individuellen Einschr√§nkungen. | ‚ÄûAlso, es muss auf jeden Fall einfach zu bedienen sein \[‚Ä¶\]‚Äú (TN_1, Zeile 75). |
| Verf√ºgbarkeit | Unter dem Kriterium der Verf√ºgbarkeit wird verstanden, dass die KI-Anwendung allen Nutzer\*innen jederzeit und unter gleichen Bedingungen zug√§nglich ist. | ‚ÄûAlso, es muss auf jeden Fall \[‚Ä¶\] f√ºr jeden gleich verf√ºgbar sein‚Äú (TN_1, Zeile 75). |
| erfassbare Aufbereitung | Die erfassbare Aufbereitung beschreibt den Anspruch, Informationen visuell so zu gestalten, dass sie schnell, klar und inhaltlich korrekt wahrgenommen und verstanden werden k√∂nnen. | ‚ÄûUnd genau, mit einfach kommt nat√ºrlich auch sprachlich, dass man es versteht und hatte ich gesagt, dass es schnell sein soll, also dass man sehr schnell versteht, was das Ergebnis von der KI ist. Also wenn das jetzt das einsch√§tzt, dass es praktisch am Anfang direkt einmal sagt, das ist richtig, oder das ist falsch, \[...\]‚Äú (TN_1, Zeile 83 - 86). |
| Einsehbarkeit der Quellen | Die Einsehbarkeit der Quellen beschreibt den Anspruch, dass alle verwendeten Informationsquellen klar benannt, zug√§nglich und auf ihre Richtigkeit √ºberpr√ºfbar sind. | ‚ÄûDas Einzige, was ich vielleicht finde, ist, dass man die Anforderungen an Quellen oder so was noch hinzuf√ºgen k√∂nnte, dass man halt sagt, ja, woher hast du das oder wie bist du darauf gekommen?‚Äú (TN_1, Zeile 123 - 125) |
| Best√§ndigkeit der Detektionsergebnisse | Unter Best√§ndigkeit der Detektionsergebnisse wird verstanden, dass die Ergebnisse der Misinformations-Detektion konsistent und unabh√§ngig von der Interaktion mit dem User bleiben. | ‚ÄûUnd deswegen finde ich es glaube ich eigentlich besser, wenn man wirklich nur die Einsch√§tzung an sich hat, die dann halt feststeht. Und nicht so, weil, also ich habe das Gef√ºhl, dass man sonst mit der KI ins Diskutieren kommt‚Äú (TN_1, Zeile 120 - 122). |
| personenunabh√§ngige Detektion | Die personenunabh√§ngige Detektion beschreibt den Anspruch, dass die Erkennung von Misinformation ohne Einbezug personenbezogene Daten erfolgt. | ‚ÄûNaja, es d√ºrfte halt nicht auf die Leute zugeschnitten sein im Idealfall \[‚Ä¶\]‚Äú (TN_2, Zeile 56). |
| politischen Unabh√§ngigkeit des KI-Tools | Unter politischer Unabh√§ngigkeit wird der Anspruch verstanden, dass die Detektionsanwendung frei von parteipolitischen Einfl√ºssen agiert und ausschlie√ülich auf Grundlage neutraler, objektiver Kriterien Inhalte auf Misinformation pr√ºft und bewertet. | ‚ÄûEs m√ºsste halt sehr unabh√§ngig programmiert sein, dass keine Lobbys oder so damit entwickeln, die halt vielleicht extra falsche Formationen verstreuen wollen‚Äú (TN_2, Zeile 57 - 59). |

: Analyseergebnisse der Interviews {#tbl-analyse}

Wie Table 1 zu entnehmen ist, konnten sieben zentrale Anforderungsbereiche identifiziert werden. So wird der Anspruch formuliert, dass die Gestaltung der KI-Anwendung allen Menschen eine gleichberechtigte und selbstst√§ndige Nutzung erm√∂glichen soll (TN_1, Zeile 75). Ebenso wird auf eine gleichberechtigte Verf√ºgbarkeit Wert gelegt, sodass die Anwendung allen Nutzenden jederzeit und unter gleichen Bedingungen zug√§nglich ist (TN_1, Zeile 75). Dar√ºber hinaus wird eine visuell-textliche Darstellungsweise gew√ºnscht, die eine schnelle, klare und inhaltlich korrekte Wahrnehmung und Verst√§ndlichkeit der bereitgestellten Informationen erm√∂glicht (TN_1, Zeile 83‚Äì86). Dieses Kriterium ist vom Anspruch der Barrierefreiheit abzugrenzen, da es nicht auf die allgemeine Zug√§nglichkeit der Anwendung, sondern auf die konkrete Darstellung der Inhalte und ihre Erfassung abzielt. Im Zentrum steht hierbei das Ergebnis der KI-Anwendung, das die Nutzenden dar√ºber informiert, ob es sich bei einem Inhalt um Misinformation handelt oder nicht. Zus√§tzlich soll eine vertiefende Auseinandersetzung mit den zugrunde liegenden Quellen m√∂glich sein, die eine freiwillige Erweiterung der KI-R√ºckmeldung darstellt (TN_1, Zeile 123‚Äì125). Zwar wird die M√∂glichkeit f√ºr R√ºckfragen zum Verst√§ndnis gew√ºnscht (TN_1, Zeile 142), dar√ºber hinausgehende Interaktionen mit der KI-Anwendung werden jedoch abgelehnt. Gefordert wird, dass das Detektionsergebnis unabh√§ngig von weiterer User-Interaktion bleibt und keine dialogische Auseinandersetzung mit der Anwendung m√∂glich ist (TN_1, Zeile 120 - 122). Diese Forderung basiert auf der Erfahrung, dass Chatbots durch Nutzende beeinflussbar sind und dadurch das Ergebnis der Detektion ver√§ndert werden k√∂nnte. Die geforderte Unabh√§ngigkeit bezieht sich weiterhin auch auf die zugrunde liegenden Daten der KI. Es soll vermieden werden, dass personenbezogene Informationen des Users in die Bewertung einflie√üen, etwa zur Anpassung an Sprachstil oder politische Haltung (TN_2, Zeile 56). Die Anwendung soll als neutrale Instanz konzipiert sein, unabh√§ngig auch von m√∂glichen Interessen staatlicher oder unternehmerischer Akteure (TN_2, Zeile 57‚Äì59). Folglich wird gefordert, dass die Anwendung frei von parteipolitischen Einfl√ºssen agiert und ausschlie√ülich auf Grundlage neutraler, objektiver Kriterien Inhalte auf Misinformation pr√ºft und bewertet. Diese Bewertung soll visuell-textlich f√ºr alle Nutzenden schnell und verst√§ndlich erfassbar sein. Es soll die M√∂glichkeit geben, die Quellen und Gr√ºnde f√ºr die Bewertung einzusehen und eigenst√§ndig zu √ºberpr√ºfen.

### Quantitative Ergebnisse

#### Stichprobe

Der Datensatz f√ºr die Analyse wurde vor einem Team vorgefiltert, eine zus√§tzliche Filterung oder Ausschlusskriterien nach Datenerhebung war nicht erforderlich. Es wurden *N* = 176 Teilnehmenden in die folgenden Analyse berr√ºcksichtigt Die Stichprobe verteilte sich nahezu gleichm√§√üig: 89 Personen bearbeiteten die Aufgabe mit einer empfehlenden KI-Variante, w√§hrend 87 Teilnehmende mit einer evaluativen Variante arbeiteten. Alle Teilnehmenden durchliefen eine Phase ohne und eine Phase mit KI-Unterst√ºtzung. Das Alter wurde metrisch erhoben. Das Durchschnittsalter betrug *M* = 25,9 (*Mdn* = 22, *SD* = 10,6). Eine obere Altersgrenze wurde nicht gesetzt, um eine m√∂glichst diverse Stichprobe zu gew√§hrleisten. So liegt das Alter der Teilnehmenden zwischen 18 und 69 Jahren. Das Geschlecht wurde nominalskaliert erfasst (m√§nnlich, weiblich, divers). 90 Teilnehmende identifizierten sich als weiblich, 84 als m√§nnlich und zwei als non-bin√§r. Auf die Frage nach der Vorerfahrung im Umgang mit K√ºnstlicher Intelligenz (metrisch) stuften neun Teilnehmende diese als sehr hoch ein, 47 als eher hoch, w√§hrend zehn Personen angaben, √ºber sehr geringe Vorerfahrung zu verf√ºgen und 25 √ºber eher geringe Vorerfahrung. Die Mehrheit von 85 Teilnehmenden bewertete ihre Vorerfahrung als mittelm√§√üig.

Die Technikaffinit√§t der Stichprobe wurde anhand der ATI-Skala (Fragebogen zur Affinit√§t gegen√ºber Technikinteraktion, @franke2019ati) erhoben. Die interne Konsistenz der Skala ergab einen Wert f√ºr Cronbach‚Äôs Alpha von Œ± = 0.91, was auf eine sehr hohe interne Konsistenz der Skala hinweist. Die Itemanalyse ergab, dass fast alle Items zu einer hohen Reliabilit√§t beitrugen. Nur das Item ati_3 zeigte mit eine vergleichsweise geringe Trennsch√§rfe, doch w√ºrde der Alpha-Wert bei Entfernung dieses Items nur geringf√ºgig auf 0.92 steigen. Da der Gesamtwert jedoch bereits sehr hoch ist und das Item theoretisch relevant erscheint, wurde entschieden, ati_3 ("In erster Linie besch√§ftige ich mich mit technischen Systemen, weil ich muss.") in der Skala zu belassen. Der durchschnittliche Skalenwert lag bei *M* = 3.7 (*SD* = 0.58), was auf eine insgesamt moderate Technikaffinit√§t der Teilnehmenden hinweist.

#### Deskriptive Statistik

Zur quantitativen Auswertung der Studie wurden zwei abh√§ngige Variablen betrachtet: die subjektive mentale Belastung sowie das Vertrauen in das verwendete KI-System. Die mentale Belastung wurde mithilfe der DLR-Workload-Skala (@dlr119443) nach der Nutzung der jeweiligen KI-Anwendung erfasst. Die Erhebung erfolgte √ºber eine visuelle Analogskala im Bereich von 0 bis 200 Punkten. Das Vertrauen in das System wurde anhand eines Einzelitems auf einer 5-Punkte-Likert-Skala gemessen (1 = sehr geringes Vertrauen, 5 = sehr hohes Vertrauen) nach @Mandsen_Gregor.

@tbl-belastung zeigt die deskriptiven Statistiken beider Variablen. Die mentale Belastung weist bei *N* = 176 einen Mittelwert von *M* = 105.90 (SD = 22.63) auf, bei einem Wertebereich von 44.38 bis 178.13. Dies deutet auf eine mittlere bis leicht erh√∂hte Beanspruchung hin. Das Vertrauen in das KI-System lag im Mittel bei *M* = 3.35 (*SD* = 0.93) und spiegelt somit eine eher neutrale bis tendenziell positive Haltung gegen√ºber der KI-Anwendung wider. Zur differenzierten Betrachtung der mentalen Belastung nach Interaktionstyp wurde zus√§tzlich eine Gruppierung nach KI-Typ vorgenommen (evaluativ vs. empfehlend). Dies wurde in @tbl-belastung zusammengefasst.

```{r}
#| label: setup
#| echo: false
#| message: false
#| warning: false

# Pakete laden
library(dplyr)
library(tidyr)
library(psych)
library(knitr)
library(kableExtra)
library(ggplot2)


# Daten einlesen
data <- read.csv("data_combined.csv", stringsAsFactors = FALSE)

# Items zur Belastung nach KI-Anwendung
ki_items <- grep("^ki_dlr_wat_", names(data), value = TRUE)

# Mittelwert berechnen (Belastung nach KI-Nutzung)
data$DLR_KI <- rowMeans(data[, ki_items], na.rm = TRUE)

# Gruppierungsvariable erstellen anhand vorhandener Werte
data$group <- NA
data$group[!is.na(data$workload_mean_E)] <- "Evaluativ"
data$group[!is.na(data$workload_mean_R)] <- "Empfehlend"

# Falls 'group' NA ist, und DLR_KI-Wert vorhanden, trotzdem mitnehmen
data_grouped <- data %>%
  filter(!is.na(group) & !is.na(DLR_KI))

# Deskriptive Statistik nach Gruppen
desc_stats_grouped <- data_grouped %>%
  group_by(group) %>%
  summarise(
    Stichprobenumfang = sum(!is.na(DLR_KI)),
    Mittelwert = mean(DLR_KI, na.rm = TRUE),
    Median = median(DLR_KI, na.rm = TRUE),
    Standardabweichung = sd(DLR_KI, na.rm = TRUE),
    Minimum = min(DLR_KI, na.rm = TRUE),
    Maximum = max(DLR_KI, na.rm = TRUE),
    .groups = "drop"
  )

#| label: tbl-belastung
#| tbl-cap: "Deskriptive Statistik der mentalen Belastung"
# Tabelle anzeigen
kable(desc_stats_grouped,
      caption = "Deskriptive Statistik der mentalen Belastung nach KI-Typ",
      booktabs = TRUE, digits = 2) %>%
  kable_styling()

```

```{r}
#| echo: false
#| message: false
#| warning: false

# Pakete laden
library(psych)
library(knitr)

# Daten laden
data <- read.csv("data_combined.csv", stringsAsFactors = FALSE)

# Items f√ºr die KI-Skala
ki_items <- grep("^ki_dlr_wat_", names(data), value = TRUE)
data$DLR_KI <- rowMeans(data[, ki_items], na.rm = TRUE)

# Vertrauen (invertiert)
data$Trust <- 7 - as.numeric(data$aiInformationDistrust)

# Cronbach's Alpha f√ºr mentale Belastung (KI)
alpha_ki <- psych::alpha(data[, ki_items], check.keys = TRUE)$total$raw_alpha


# Zusammenfassung 
summary_table <- data.frame(
  Variable = c("Mentale Belastung", "Vertrauen"),
  Stichprobenumfang = c(sum(!is.na(data$DLR_KI)), sum(!is.na(data$Trust))),
  Mittelwert = c(mean(data$DLR_KI, na.rm = TRUE), mean(data$Trust, na.rm = TRUE)),
  Median = c(median(data$DLR_KI, na.rm = TRUE), median(data$Trust, na.rm = TRUE)),
  Standardabweichung = c(sd(data$DLR_KI, na.rm = TRUE), sd(data$Trust, na.rm = TRUE)),
  Minimum = c(min(data$DLR_KI, na.rm = TRUE), min(data$Trust, na.rm = TRUE)),
  Maximum = c(max(data$DLR_KI, na.rm = TRUE), max(data$Trust, na.rm = TRUE)),
  Cronbachs_Alpha  = c(round(alpha_ki, 3),"-")
)

#| label: tbl-belastung-vertrauen
#| tbl-cap: "Deskriptive Statistik f√ºr mentale Belastung und Vertrauen"

# Tabelle 
kable(summary_table, caption = "Deskriptive Statistik f√ºr mentale Belastung und Vertrauen", digits = 3)

```

#### Inferenzstatistik

Die durchschnittliche mentale Belastung unterschied sich nicht signifikant zwischen Teilnehmenden in der Bedingung ‚ÄûEmpfehlend‚Äú (*M* = 104.85, *SD* = 20.31) und der Bedingung ‚ÄûEvaluativ‚Äú (*M* = 106.97, *SD* = 20.87), *t*(173.63) = -0,62, *p* = .536 und *d* = 0.09. Die Effektst√§rke entspricht nach Cohen (1988) keinem Effekt. Dies deutet darauf hin, dass es keinen bedeutsamen Unterschied in der empfundenen mentalen Belastung zwischen den beiden KI-Typen gibt. Dies ist im Dot-and-Whisker-Plot @fig-belastung-plot veranschaulicht.

```{r}
#| echo: false
#| message: false
#| warning: false


# Unabh√§ngiger t-Test (Welch-Test) f√ºr mentale Belastung
t_test_result <- t.test(DLR_KI ~ group,
                        data = data_grouped,
                        var.equal = FALSE)

# Daten f√ºr Plot vorbereiten
plot_data <- data_grouped %>%
  group_by(group) %>%
  summarise(
    mean = mean(DLR_KI, na.rm = TRUE),
    sd = sd(DLR_KI, na.rm = TRUE),
    n = n(),
    se = sd / sqrt(n),
    ci_lower = mean - qt(0.975, df = n - 1) * se,
    ci_upper = mean + qt(0.975, df = n - 1) * se,
    .groups = "drop"
  )

#| label: fig-belastung-plot
#| fig-cap: "Mentale Belastung nach KI-Typ"

# Dot-and-Whisker-Plot 
ggplot(plot_data, aes(x = group, y = mean)) +
  geom_point(size = 4, color = "#2C3E50") +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.1, color = "#2C3E50") + scale_y_continuous(limits = c(95, 115)) +
  labs(
    title = "Abbildung: Mentale Belastung nach KI-Typ",
    x = "KI-Typ",
    y = "Mentale Belastung (0‚Äì200)"
  ) +
  theme_minimal(base_size = 14)

#| echo: false
#| message: false
#| warning: false

```

Zudem wurde eine Korrelationsanalyse nach Pearson durchgef√ºhrt, um den Zusammenhang zwischen der subjektiv empfundenen mentalen Belastung und dem Vertrauen in das KI-System zu untersuchen. Die Analyse ergab keinen signifikanten Zusammenhang (*r*(174) = .089, *p* = .238). Dieser Wert f√ºr Pearson‚Äôs r entspricht laut @bourier_zusammenhang_2025 keinem Effekt. Es zeigt sich zwar eine tendenziell positive Beziehung, jedoch ohne ausreichende statistische Evidenz. Entsprechend kann nicht belegt werden, dass eine h√∂here empfundene kognitive Belastung mit einem geringeren Vertrauen in das System einhergeht. Diese Beziehung wird in @fig-korrelation-belastung-vertrauen, dem Scatter-Plot mit Regressionslinie, grafisch dargestellt. Die Punktwolke zeigt eine leichte positive Tendenz, unterst√ºtzt durch die eingef√ºgte Regressionslinie.

```{r}
#| echo: false
#| message: false
#| warning: false

#| label: fig-korrelation-belastung-vertrauen
#| fig-cap: "Zusammenhang: mentaler Belastung und Vertrauen"
# Scatter-Plot
ggplot(data, aes(x = DLR_KI, y = Trust)) +
  geom_jitter(width = 0.2, height = 0.2, color = "#0072B2", alpha = 0.6, size = 2) +
  geom_smooth(method = "lm", se = FALSE, color = "#D55E00") +
  labs(
    title = "Zusammenhang: mentale Belastung (DLR) und Vertrauen",
    x = "Mentale Belastung (DLR-Skala, mit KI)",
    y = "Vertrauen in das KI-System"
  ) +
  theme_minimal(base_size = 14)

# Korrelations-Test
cor_test <- cor.test(data$DLR_KI, data$Trust, method = "pearson")
```

Zusammenfassend l√§sst sich festhalten, dass sich die beiden getesteten KI-Systeme in ihrer Auswirkung auf die mentale Belastung nicht signifikant voneinander unterschieden. Beide Varianten f√ºhrten zu vergleichbaren Belastungswerten, was darauf hinweist, dass der eingesetzte KI-Typ ‚Äì evaluativ oder empfehlend ‚Äì keinen entscheidenden Einfluss auf das subjektive Belastungsempfinden hatte. Zudem ergab die Korrelationsanalyse zwischen der empfundenen mentalen Belastung und dem Vertrauen in das System keinen signifikanten Zusammenhang. Teilnehmende, die das System als st√§rker belastend wahrnahmen, berichteten zwar tendenziell ein geringeres Vertrauen in die KI, diese Beziehung war jedoch statistisch nicht belastbar. Diese Ergebnisse sind insbesondere f√ºr die nutzerzentrierte Gestaltung von KI-Anwendungen bedeutsam, da sie nahelegen, dass eine wahrgenommene kognitive Entlastung potenziell das Vertrauen der Nutzenden in ein KI-System f√∂rdern kann, auch wenn dies in der vorliegenden Stichprobe nicht eindeutig best√§tigt wurde.

## Diskussion

-   Beantwortung der Forschungsfrage(n).

<!-- -->

-   Wissenschaftliche/gesellschaftliche Implikationen.

-   Limitationen der Studie.

-   Zuk√ºnftige Arbeit.

    üí° Der Abschnitt ‚ÄûDiskussion‚Äú dient der Interpretation der Ergebnisse.

Was bedeuten die Ergebnisse in Bezug auf die Forschungsfrage(n)?

-   kurze Zusammenfassung der Studie, jeweils ein bis zwei S√§tze zu (Ziel, Methode, Ergebnisse) (ausf√ºhrlicher, sodass auch verstanden werden kann, wenn der Rest des extended Abstracts nicht gelesen wurde)

-   explizite Antwort der Forschungsfrage (super f√ºr R√ºckgriff)

Was l√§sst sich aus der Studie schlussfolgern (Wissenschaft/Gesellschaft)?

-   Auwirkung auf Wissenschaft (Widerspr√ºche (gr√ºnde?), Erweiterung, Nutzungsm√∂glichkeit)

-   erkl√§ren Ergebnisse Ph√§nomene/menschliches Verhalten in bestimmten Situationen?

-   unterst√ºtzend f√ºr Implementierung von Gesetzen/Richtlinien?

-   z.B. Ergebnisse legen nahe, dass bestimmte Features Mehrwert f√ºr Weiterentwicklung von KI-Tools zur Detektion von Misinformation...

Was sind die Einschr√§nkungen der Studie?

-   Zu beachten ist, dass die Stichprobe nicht repr√§sentativ war, da an einer Universit√§t √ºber einen E-Mail-Verteiler rekrutiert worden ist. Insofern handelt es sich wom√∂glich um eine selbst-selektierte Stichprobe, die mehr Interesse an Nachhaltigkeit als die allgemeine Population in Deutschland ... siehe Studierende √§hnlicher Kohorte, Technik schwerpunkt, nicht rep√§sentativ f√ºr allgemeine Population (dazu Websiten unspezifisch, einbindung in Kontext noch mal speziellere Anforderungen, hier soweit Grundlagenuntersuchung, nicht konkreter Kontext)

-   Beschr√§nkung hier Modell textliche Misinformation, nicht jede Form textuell;

-   Zeitpunkt, politisch Misinformation und gro√üer Boom wie Skepsis KI?

Was sind, auf Basis der Implikationen und Limitationen, sinnvolle Folgestudien?

## Code of Conduct {.unnumbered}

Als Arbeitsgruppe verpflichten wir uns einem gemeinsamen Verhaltenskodex, der den Anforderungen wisschenschaftlicher Forschung und respektvollen Umgang entspricht. Dieser bildet die Grundlage f√ºr die Zusammenarbeit und beinhaltet folgende Punkte:

Wir begegenen einander offen und respektvoll. Konstruktives Feedback wird wertgesch√§tzend ge√§u√üert, aufgenommen und besprochen. Dabei sind unterschiedliche Perspektiven und Meinungen willkommen und werden als zur Zusammenarbeit geh√∂rend betrachtet. Entsprechend wird sich im Konfliktfall gemeinsam bem√ºht wird, eine zufriedenstellende L√∂sung zu finden. Die Mitglieder werden als gleichberechtig angesehen und behandelt.

Es wird eine ausgewogene Aufteilung der Arbeitslast angestrebt, die die St√§rken und Schw√§chen der einzelnen Mitgleider ber√ºcksichtigt. Folglich verpflichten wir uns zu einer offenen und direkten Kommunikation. Im Falle m√∂glicher Verz√∂gerungen, Probleme oder unerwarteter Auslastungen wird fr√ºhzeitig kommuniziert und sich gegenseitig unterst√ºtzt.

Wir verpflichten uns der Einhaltung wissenschaftlicher Standards, mit besonderer Umsicht gegen√ºber erhobener, personenbezogener Daten und der Wahrung ihrer Vertraulichkeit. Ebenso wird streng auf sorgf√§ltigen Recherche und korrekte Zitation geachtet. Die verantwortungsvolle Nutzung von AI Tools erfolgt transparent mit Beschreibung und Kennzeichnung der Nutzung. So wurde @openai2025chatgpt genutzt, um die Formulierungen zu pr√§zisieren, Code zu √ºberpr√ºfen und die Rechtschreibung zu kontrollieren.

## Literaturverzeichnis {.unnumbered}

## Anhang 1 - Rekrutierungstext {.unnumbered}

**Online-Studie zur Erkennung von Falschinformationen ‚Äì Teilnehmende gesucht!**

Wir f√ºhren im Rahmen eines Forschungsmoduls eine **Online-Umfrage** durch (Dauer: ca. **60 Minuten**). In der Studie wird untersuchen, wie verschiedene KI-gest√ºtzte Systeme Menschen dabei unterst√ºtzen k√∂nnen, Falschinformationen zu erkennen.

G**esucht werden Teilnehmende, die...**

\- mindestens **18 Jahre alt** sind,

\- **nicht** an der Veranstaltung **SMNF** teilnehmen,

\- **Social Media zumindest gelegentlich nutzen**,

\- **Deutsch** sprechen und verstehen,

\- Zugriff auf ein **Tablet, Laptop oder PC** haben,

\- einen **modernen Browser** nutzen (z.B. Chrome, Firefox oder Safari).

**Wichtig:** Alle erhobenen Daten sind **vollst√§ndig anonym** und **nicht auf Sie r√ºckf√ºhrbar**.

Interesse? Hier geht's direkt zur Studie:

https://dsslab.hciuse.sh/study/pilot?groupId=gr-a2

Danke f√ºr Ihre Unterst√ºtzung!
