---
title: "KI-Systeme zur Erkennung von Misinformation: Was fordern Nutzende?"
author:
  - "Carys Artemis Lamm"
  - "Lara Senger"
  - "Mino Wittemann"
date: today
date-format: "D. MMMM YYYY"
format:
  pdf:
    papersize: a4
    fontsize: 12pt
    number-sections: true
    fig-cap-location: top
    fig-numbering: true
    tbl-numbering: true
    code-fold: true
  html:
    fontsize: 12pt
    number-sections: true
    fig-numbering: true
    tbl-numbering: true
    code-fold: true
lang: de
bibliography: literatur.bib
csl: apa7.csl
---

**GitHub Repository:** <https://github.com/MissDoctrix/Statistik-A2.git>\
**Für die Abgabe aktueller GitHub Hash:** 2b414e8d0478ee4c6328d2f6641749a22a015246

to Do:

-   Diskussion

-   textliche Verknüpfung und Stilanpassung

-   Rückmeldung Ergebnisse (dynamische Referenzierung, Scatterplot (?),)

-   Abschlusspräsentation

    -   Einleitung, inkl. Einführung, Relevanz und Zielsetzung, 3min (done)

    -   Methodik, inkl. Arten der Erhebungen und Auswertungsstrategie, 1,5min

    -   Ergebnisse (qual. + quant.) inkl. Diskussion der Implika onen und Relevanz, 7,5min

    -   Diskussion der Limitationen und möglicher zukünftiger Arbeiten, 3min

## Einleitung

Misinformationen auf social Media stellen eine wachsende Herausforderung für Nutzende dar. Im Jahr 2024 nutzten laut der ARD/ZDF-Medienstudie etwa 60 % der deutschen Bevölkerung ab 14 Jahren soziale Netzwerke (@müller2024). Diese Plattformen dienen zunehmend als Informationsquellen, wobei es für die Nutzende schwierig ist, falsche oder irreführende Inhalte zu erkennen. Derartige Inhalte können gesellschaftliche Spannungen verschärfen und politische sowie wirtschaftliche Prozesse beeinflussen (@gesing_desinformation_2024). Spätestens seit dem US-amerikanischen Präsidentschaftswahlkampf im Jahr 2016 gilt die gezielte Verbreitung von Falschinformationen über social Media als strategisches Mittel zur Meinungsbeeinflussung (@rudloff_fake_2023).

In der Forschung finden sich verschiedene Begriffe wie „Fake News“, „Desinformation“ oder „Misinformation“, die häufig synonym verwendet werden. Im Rahmen dieser Studie verwenden wir den Begriff Misinformation und orientieren uns dabei an @rudloff_fake_2023, den zufolge es sich um frei erfundene, irreführende oder manipulierte Inhalte handelt.

Die Auseinandersetzung mit Misinformation ist kaum zu vermeiden. Sie verbreiten sich sowohl über private Kommunikationskanäle wie Messenger-Dienste als auch algorithmisch verstärkt durch social Media Plattfordem, auf welchen polarisierende Beiträge besonderes Sichtbarkeit erlangen. Die kritische Bewertung solcher Inhalte gestaltet sich aus verschiedenen Gründen als schwierig: Menschen neigen dazu, Informationen so zu interpretieren, dass sie ihr bestehendes Weltbild bestätigen, und sie überschätzen die Wahrscheinlichkeit wahrer Inhalte aufgrund wiederholter Konfrontation (@rudloff_fake_2023). Diese kognitiven Mechanismen wirken sich insbesondere in schnelllebigen, auf kurze Aufmerksamkeitsspannen ausgelegten social Media aus.

Vor diesem Hintergrund setzt aktuelle Forschung im Bereich der algorithmischen Erkennung von Misinformation an. Die Entwicklung der sogenannten Künstlichen Intelligenz (im Folgenden: KI) nimmt hierbei eine zentrale Rolle ein. KI-basierte Modelle wie FakeBERT (@kaliyar_fakebert_2021) oder exBAKE (@jwa_exbake_2019) weisen bereits eine hohe Detektionsgenauigkeit auf. Eine verlässliche Detektion allein reicht jedoch nicht aus, um Nutzende im praktischen Umgang mit Misinformation effektiv zu unterstützen.

Ebenso ist entscheidend, wie entsprechende Systeme gestaltet sind und in welcher Weise Menschen mit ihnen interagieren. Erste Studien heben hervor, dass insbesondere transparente Bewertungen, nachvollziehbare Begründungen von Entscheidungen sowie die Einbindung glaubwürdiger Quellen zentrale Anforderungen darstellen (@amri_afcc_2024; @shu_defend_2019; @szczepanski_new_2021). Dennoch beruhen viele dieser Erkenntnisse bislang überwiegend auf theoretischen Annahmen, systematische empirische Untersuchungen mit Fokus auf Nutzende fehlen weitgehend.

Ziel der vorliegenden Studie ist es daher, ausgehend von den Nutzende, konkrete Anforderungen an KI-gestützte Tools zur Erkennung von Misinformation zu identifizieren und Faktoren zu bestimmen, die eine erfolgreiche Nutzung begünstigen. Im Zentrum steht dabei die folgende Forschungsfrage: Welche Anforderungen stellen Nutzende an eine KI-Anwendung zur Erkennung von Misinformation auf Social Media?

Zur Beantwortung dieser Frage kombinieren wir qualitative Leitfadeninterviews mit einer quantitativen mixed Design Umfrage. Der Aufsatz ist in folgende Hauptabschnitte gegliedert: Einleitung, Literaturübersicht, Methodik, Ergebnisse, Diskussion und Fazit. Die Einleitung führt in das Thema ein und stellt die Relevanz und Zielsetzung dar. Die Literaturübersicht fasst bestehende Studien und Theorien zusammen und identifiziert Forschungslücken. Im Methodikteil wird das Vorgehen für die Leitfadeninterviews und die Gestaltung der digitalen Umfrage ausgeführt. Anschließend werden die Analyseergebnisse der Interviews und der Umfrage im Ergebnisteil erläutert. Zum Schluss werden die Erkenntnisse im Diskussionsteil zusammengefasst und eingeordnet. Zudem findet sich im Anhang unser Code of Conduct, die Literatur und der Rekrutierungstext für die Online-Studie.

## Literaturübersicht

Es stehen zwei Fragen im Fokus der Literaturrecherche: Erstens, was sind Nutzendenanforderungen an künstliche Intelligenz als Werkzeug zur Unterstützung bei der Detektion von Misinformation auf Social Media? Zweitens, wie gehen Personen in diesem Kontext mit unterschiedlichen Feedback-Arten um?

Einen ersten Einblick in den wissenschaftlichen Forschungsstand und in die Thematik bieten @szczepanski_new_2021. Ihre Arbeit präsentiert eine Erklärungsmethode, mit der deep learning Fake‑News-Modelle nachvollziehbarer werden, indem es die Textstellen hervorhebt, die das Urteil am stärksten beeinflussen. Diese Untersuchung bietet dabei einen Ansatz für die Frage, wie die Ausgabe von KI Werkzeuge gestaltet werden kann, allerdings beinhaltet die Arbeit keine Untersuchung mit verschiedenen Nutzergruppen und ihrem Feedback.

Ähnlichen Einblick bietet @reis_explainable_2019, deren Arbeit ebenfalls die Erklärbarkeit von KI Modellen zur Fake-News-Detektion fokussiert. Durch Verwendung unterschiedlichen Daten versucht das Modell eine ganzheitliche Bewertung zu erreichen und diese nachvollziehbar für Nutzende zu machen, indem das System hervorhebt, welche Merkmale am meisten zu einer Entscheidung betragen. Allerdings liegt auch hier keine direkte Nutzerforschung vor, die Bedürfnisse der Nutzenden wurden aus Literatur extrahiert und analysiert, jedoch nicht nicht durch eine testende Studie überprüft.

Eine andere Arbeit von @shu_defend_2019 evaluiert in einer Nutzendenstudie, wie gut Erklärungen und Kommentare bezüglich der Entscheidung einer KI, ob Nachrichteninhalte Misinformationen sind, verstanden werden. Dies sind wichtige Orientierungspunkte für die Forschungsfragen. Jedoch untersucht die Arbeit nicht weitergehend, wie unterschiedliche Feedbackformen von den Nutzenden bewertet werden und wie sehr ihnen Vertrauen entgegen gebracht wird.

Andere Forschungsarbeiten fokussieren sich auf die Frage, wie Nutzende Misinformation-Detektionstool wahrnehmen. In einer experimentellen Online-Studie von @ribes_trust_2021 wurden verschiedene Layouts und Szenarien entwickelt, umgesetzt und mit 266 Teilnehmern getestet. Die Ergebnisse legen nahe, dass das Layout einen signifikanten Einfluss darauf hat, wie wichtig Nutzende die Quellenangabe eines Mediums einschätzen. Zudem wirkt sich nach der Studie die Menge der bereitgestellten Informationen zur Erklärung der KI negativ auf das Verständnis der Nutzende aus. Ein zentraler Faktor für die Akzeptanz KI-basierter Detektionssysteme stellt Vertrauen dar, identifiziert unter anderen eine Studie von @Shin2023518. Diese stellt fest, dass Vertrauen vor allem durch wahrgenommene Kompetenz, Kooperationsfähigkeit und Autonomie der Anwendung entsteht. Dies unterstreicht sowohl die Bedeutung als auch die Herausforderungen, ein Detektionstool für Nutzende verständlich zu gestalten.

Die fünf genannten Forschungsarbeiten bieten einen ersten Ansatz, sich mit dem Themenfeld vertraut zu machen. Jedoch müssen mehr wissenschaftliche Veröffentlichungen mit dem Schwerpunkt auf Wahrnehmung und Beurteilung unterschiedlicher Feedbacksysteme durch die Nutzenden herangezogen werden, die zudem den Fokus aus Nutzendentests legen und diese Untersuchungen vergleichen. Ebenso sind mehr Arbeiten zu suchen, die empirisch untersuchen, welche Anforderungen durch die Nutzenden gestellt werden und ob sich diese in bestehenden Modellen bereits wiederfinden.

Anknüpfend an die Literatur liegt unser Fokus auf der Analyse konzeptioneller und gestalterischer Anforderungen, die Nutzende an eine KI-Anwendung stellen.

## Methode

### Qualitative Methode

Um zu untersuchen, welche Anforderungen Nutzende an eine KI-Anwendung zur Erkennung von Misinformation auf Social Media stellen, wurde die qualitative Forschungsmethode des leitfadengestützten Interviews gewählt. Personeninterviews ermöglichen es, Themen aus Sicht der Befragten zu identifizieren und diese in weiteren Analyseschritten zu vertiefen. Durch offen formulierte Fragen können individuelle Perspektiven, relevante Erfahrungen und subjektive Einschätzungen eingebracht werden, die bei standardisierten Frageformaten möglicherweise unberücksichtigt bleiben. Anstatt Themen vorzugeben, erlaubt die Interviewmethode, relevante Inhalte direkt aus den Antworten der Teilnehmenden abzuleiten und für die Bearbeitung der Forschungsfrage nutzbar zu machen.

Für die Stichprobe wurden gezielt junge Erwachsene ausgewählt, die regelmäßig social Media nutzen und bereits Erfahrungen mit künstlicher Intelligenz gesammelt haben. Insbesondere sind ihnen Large Language Models wie ChatGPT bekannt und sie haben diese bereits genutzt. Die Stichprobe weist somit relevantes Vorwissen hinsichtlich der Funktionsweise sowie der Interaktionsmöglichkeiten mit KI-Systemen auf. Die Interviews orientierten sich an einem thematischen Leitfaden und umfassten Fragen zu Vorerfahrungen zu KI, Nutzungsverhalten bezüglich Social Media sowie Erwartungen an eine KI-Anwendung, die über erkannte Misinformation informiert. Insgesamt wurden zwei leitfadengestützte Interviews durchgeführt und analysiert. Die Gespräche wurden über eine mobile Applikation lokal auf dem Gerät aufgezeichnet. Die Transkripte wurden zunächst mithilfe der Software @whispertranscribe2025 erstellt und anschließend von einer zweiten Person korrigiert und formal überarbeitet.

Die Auswertung erfolgte anhand der sechsstufigen thematischen Analyse. Zur Strukturierung wurde im Programm Excel von @microsoft_excel_2024 eine Tabelle angelegt, in der Codes thematisch geordnet, zu Kategorien zusammengeführt und mit exemplarischen Zitaten aus den Interviews verknüpft wurden. Die Tabelle diente zugleich als Arbeitsgrundlage zur Überprüfung, Anpassung und finalen Formulierung der Themen und Definitionen. In mehreren Korrekturschleifen wurden diese überarbeitet und zu einem gemeinsamen Analyseergebnis verdichtet.

### Quantitative Methode

Als quantitative Untersuchung wurde eine Online-Befragung durchgeführt. Für die Rekrutierung der Teilnehmenden wurden insgesamt sechs Einschluss- bzw. Ausschlusskriterien definiert. Zur Sicherstellung der rechtlichen Einwilligung in die Verarbeitung personenbezogener Daten haben wir ein Mindestalter von 18 Jahren festgelegt. Eine obere Altersgrenze haben wir nicht gesetzt, um eine möglichst vielfältige Stichprobe zu ermöglichen. Ausgeschlossen wurden Personen, die aktuell an der Veranstaltung Statistik und Methoden der Nutzerforschung teilnehmen, da ihnen durch die Lehrveranstaltung potenziell Vorwissen zum Studiendesign vorliegt, sie somit nicht unvoreingenommen die Befragung bearbeiten können.Die Befragung wurde ausschließlich in deutscher Sprache durchgeführt. Entsprechend wurden nur deutschsprachige Personen als Teilnehmende gesucht, um Verständnisschwierigkeiten vermeiden. Darüber hinaus war für die Teilnahme der Zugriff Tablet, Laptop oder PC mit aktuellem Browser erforderlich, da die Online-Befragung ausschließlich für Desktop-Oberflächen konzipiert wurde und eine kompatible Anzeigeumgebung benötigt. Ein weiteres Einschlusskriterium war die zumindest gelegentliche Nutzung von social Media, da die Untersuchung auf Erfahrungen und Einschätzungen im Kontext von social Media Bezug nehmen. Vorerfahrungen mit KI-Systemen haben wir hingegen bewusst nicht vorausgesetzt, da die KI unterstützte Anwendung kein Vorwissen benötigt. Dadurch sollte ein bereiteres Bild hinsichtlich der unterschiedlichen Technikaffinität und des Vorwissens ermöglicht werden. Die Befragung wurde im Zeitraum vom Samstag, 24. Mai 2025, bis zum Montag, 2. Juni 2025 durchgeführt. Die Rekrutierung der Teilnehmenden erfolgte sowohl mündlich im persönlichen Umfeld als auch durch die Verbreitung mittels eines Rekrutierungstextes. Es wurden so viele Teilnehmende wie möglich aus dem näheren Bekanntenumfeld gesucht mit einer Mindestgrenze von 15 Teilnehmenden. Die Befragung wurde von den Teilnehmenden eigenständig online bearbeitet.

Die Studie folgte einem mixed factorial design, mit between-subjects-Faktoren als auch within-subjects-Faktoren. Alle Teilnehmenden führten zunächst eine Bewertung von Social Media Posts ohne Unterstützung durch KI durch (within-subjects-Faktor), gefolgt von einer Bewertung mit KI-Unterstützung. Die Teilnehmenden wurden randomisiert einer von zwei Varianten eines KI-Systems zugewiesen (between-subjects-Faktor). Die eine Häfte bewertete eine evaluierende, die andere eine empfehlende KI-Variante.

------------------------------------------------------------------------

![Ablauf der quantitativen Studie](images/Untitled.png){#fig-ablauf-studie}

Die Datenerhebung bestand aus insgesamt fünf Abschnitten (wie in @fig-ablauf-studie vollständig dargestellt): einem Pre-Test, einer Baseline-Phase, einem Post-Baseline-Fragebogen, dem Test der Anwendung und einem abschleißenden Post-Test-Fragebogen. Im Pre-Test wurden grundlegende demografische Daten erfasst, darunter das Alter der Teilnehmenden (metrisch), das Geschlecht (nominalskaliert: männlich, weiblich, divers) sowie der Bildungsstand (ordinalskaliert mit mehreren Bildungsstufen). Zusätzlich wurden Angaben zur Technikaffinität (metrisch), zur Nutzung von social Media (teils ordinal und teils nominal) sowie zum Vorwissen über KI (metrisch) erfasst. Im Hauptteil der Studie mussten die Teilnehmenden 20 Social-Media-Beiträge bewerten, die potentiell Falschinformationen enthielten. Die Beiträge waren in zwei Phasen aufgeteilt: In der ersten Phase (Baseline) wurden zehn Beiträge ohne Unterstützung durch ein KI-System bewertet. In der zweiten Phase (Test der Anwendung) erfolgte die Bewertung von zehn weiteren Beiträgen, hier mit Unterstützung durch eines von zwei KI-Systemen: entweder ein evaluatives System, das eine eigene Einschätzung abgibt, oder ein empfehlendes System, das eine Handlungsempfehlung formuliert. Die Zuordnung der KI-Systeme erfolgte zwischen den Teilnehmenden randomisiert. Zu jedem Beitrag sollten die Teilnehmenden einschätzen, ob es sich um Falschinformationen handelt (ja/nein). Diese Einschätzung wurde binär erfasst (nominalskaliert). Parallel wurde erfasst, unter welcher Systembedingung der jeweilige Beitrag präsentiert wurde: entweder ohne Assistenz (Baseline), mit einer evaluativen Entscheidungshilfe oder mit einer direkten Empfehlung. Nach jeder der beiden Bewertungsphasenwurde ein Post-Test durchgeführt, in dem die subjektiv empfundene mentale Belastung abgefragt wurde. Dafür kam hier eine Skala zum Einsatz, die sich an der DLR-Workload-Skala orientiert. Diese Skala misst fünf Bereiche der mentalen Beanspruchung: Informationsaufnahme, Wissensabruf, Entscheidungsfindung, motorische Beanspruchung und zeitlicher Druck. Jeder Bereich wurde einzeln auf einer visuellen Analogskala von 0 (sehr stark unterbeansprucht) bis 200 (sehr stark überbeansprucht) bewertet, wobei 100 als optimaler Wert gilt. Die Teilnehmenden konnten somit sehr differenziert angeben, wie belastend sie die Aufgaben empfanden. Die DLR-Skala gilt als ökonomisch, da sie trotz weniger Fragen eine differenzierte Erfassung ermöglicht. Die binäre Entscheidung bei den Beiträgen (ja/nein) ist angemessen für unser Ziel, um herauszufinden, wie sicher und richtig Teilnehmende Falschinformationen erkennen. Insgesamt liefert der Studiendesign sowohl objektive Daten (Entscheidung pro Beitrag), als auch subjektive Einschätzungen (über die Belastung). Damit lässt sich analysieren, ob und wie die KI-Systeme einen Einfluss auf das Erkennen von Misinformation haben.

Zur Analyse der erhobenen Daten sind zwei zentrale statistische Verfahren geplant: Ein t-Test für unabhängige Stichproben sowie eine Korrelationsanalyse nach Pearson. Dadurch können sowohl Gruppenunterschiede als auch die Zusammenhänge zwischen ausgewählten Variablen untersucht werden. Zunächst wird zur Untersuchung von Gruppenunterschieden ein t-Test für unabhängige Stichproben eingesetzt. Dieser Test prüft, ob sich die mittlere subjektive Belastung (erhoben mit der DLR Workload-Skala nach @dlr119443) in Abhängigkeit von der Art der KI-Unterstützung (evaluativ vs. empfehlend) signifikant unterscheidet. Die Belastung wird dabei als metrisch skalierte Variable behandelt, da sie auf einer visuellen Analogskala von 0 bis 200 erhoben wurde. Die Gruppenzuordnung (Art der KI) ist nominalskaliert, weshalb sich der t-Test als geeignete Methode zur Prüfung dieser Hypothese anbietet. Das Ziel dieser Analyse ist es herauszufinden, ob eine bestimmte Form der KI-Assistenz kognitiv entlastender wirkt als die andere, was für die Gestaltung zukünftiger Systeme relevant sein könnte. Ergänzend dazu wird eine Korrelation nach Pearson durchgeführt, um mögliche Zusammenhänge zwischen der subjektive empfundenen Belastung und dem Vertrauen in das KI-System zu untersuchen. Das Vertrauen wurde im Post-Test mit Hilfe mehreren Aussagen auf einer metrisch interpretierten Likert-Skala erfasst (@Mandsen_Gregor). Da beide Variablen (subjektive Belastung und Vertrauen) als intervallskaliert gelten, ist der Pearson-Korrelationskoeffizient eine geeignete Methode zur Analyse. Diese Analyse soll zeigen, ob beispielsweise eine höhere Belastung mit einem niedrigeren Vertrauen in das System einhergeht. Für beide Analysen wird ein Signifikanzniveau von 5% (α = 0.05) festgelegt.

## Ergebnisse

### Qualitative Ergebnisse

Im Rahmen der Interviews konnten mehrere thematische Schwerpunkte identifiziert werden, die von den Befragten als zentrale Anforderungen an eine KI-Anwendung zur Erkennung von Misinformation auf Social Media formuliert wurden.

| Name | Definition | Textstelle |
|:-----------------------|:-----------------------|:-----------------------|
| Barrierefreiheit | Die Barrierefreiheit beschreibt den Anspruch, dass die Gestaltung der Detektion allen Menschen eine gleichberechtigte und selbstständige Nutzung ermöglicht, unabhängig von individuellen Einschränkungen. | „Also, es muss auf jeden Fall einfach zu bedienen sein \[…\]“ (TN_1, Zeile 75). |
| Verfügbarkeit | Unter dem Kriterium der Verfügbarkeit wird verstanden, dass die KI-Anwendung allen Nutzer\*innen jederzeit und unter gleichen Bedingungen zugänglich ist. | „Also, es muss auf jeden Fall \[…\] für jeden gleich verfügbar sein“ (TN_1, Zeile 75). |
| erfassbare Aufbereitung | Die erfassbare Aufbereitung beschreibt den Anspruch, Informationen visuell so zu gestalten, dass sie schnell, klar und inhaltlich korrekt wahrgenommen und verstanden werden können. | „Und genau, mit einfach kommt natürlich auch sprachlich, dass man es versteht und hatte ich gesagt, dass es schnell sein soll, also dass man sehr schnell versteht, was das Ergebnis von der KI ist. Also wenn das jetzt das einschätzt, dass es praktisch am Anfang direkt einmal sagt, das ist richtig, oder das ist falsch, \[...\]“ (TN_1, Zeile 83 - 86). |
| Einsehbarkeit der Quellen | Die Einsehbarkeit der Quellen beschreibt den Anspruch, dass alle verwendeten Informationsquellen klar benannt, zugänglich und auf ihre Richtigkeit überprüfbar sind. | „Das Einzige, was ich vielleicht finde, ist, dass man die Anforderungen an Quellen oder so was noch hinzufügen könnte, dass man halt sagt, ja, woher hast du das oder wie bist du darauf gekommen?“ (TN_1, Zeile 123 - 125) |
| Beständigkeit der Detektionsergebnisse | Unter Beständigkeit der Detektionsergebnisse wird verstanden, dass die Ergebnisse der Misinformations-Detektion konsistent und unabhängig von der Interaktion mit dem User bleiben. | „Und deswegen finde ich es glaube ich eigentlich besser, wenn man wirklich nur die Einschätzung an sich hat, die dann halt feststeht. Und nicht so, weil, also ich habe das Gefühl, dass man sonst mit der KI ins Diskutieren kommt“ (TN_1, Zeile 120 - 122). |
| personenunabhängige Detektion | Die personenunabhängige Detektion beschreibt den Anspruch, dass die Erkennung von Misinformation ohne Einbezug personenbezogene Daten erfolgt. | „Naja, es dürfte halt nicht auf die Leute zugeschnitten sein im Idealfall \[…\]“ (TN_2, Zeile 56). |
| politischen Unabhängigkeit des KI-Tools | Unter politischer Unabhängigkeit wird der Anspruch verstanden, dass die Detektionsanwendung frei von parteipolitischen Einflüssen agiert und ausschließlich auf Grundlage neutraler, objektiver Kriterien Inhalte auf Misinformation prüft und bewertet. | „Es müsste halt sehr unabhängig programmiert sein, dass keine Lobbys oder so damit entwickeln, die halt vielleicht extra falsche Formationen verstreuen wollen“ (TN_2, Zeile 57 - 59). |

: Analyseergebnisse der Interviews {#tbl-analyse}

Wie Table 1 zu entnehmen ist, konnten sieben zentrale Anforderungsbereiche identifiziert werden. So wird der Anspruch formuliert, dass die Gestaltung der KI-Anwendung allen Menschen eine gleichberechtigte und selbstständige Nutzung ermöglichen soll (TN_1, Zeile 75). Ebenso wird auf eine gleichberechtigte Verfügbarkeit Wert gelegt, sodass die Anwendung allen Nutzenden jederzeit und unter gleichen Bedingungen zugänglich ist (TN_1, Zeile 75). Darüber hinaus wird eine visuell-textliche Darstellungsweise gewünscht, die eine schnelle, klare und inhaltlich korrekte Wahrnehmung und Verständlichkeit der bereitgestellten Informationen ermöglicht (TN_1, Zeile 83–86). Dieses Kriterium ist vom Anspruch der Barrierefreiheit abzugrenzen, da es nicht auf die allgemeine Zugänglichkeit der Anwendung, sondern auf die konkrete Darstellung der Inhalte und ihre Erfassung abzielt. Im Zentrum steht hierbei das Ergebnis der KI-Anwendung, das die Nutzenden darüber informiert, ob es sich bei einem Inhalt um Misinformation handelt oder nicht. Zusätzlich soll eine vertiefende Auseinandersetzung mit den zugrunde liegenden Quellen möglich sein, die eine freiwillige Erweiterung der KI-Rückmeldung darstellt (TN_1, Zeile 123–125). Zwar wird die Möglichkeit für Rückfragen zum Verständnis gewünscht (TN_1, Zeile 142), darüber hinausgehende Interaktionen mit der KI-Anwendung werden jedoch abgelehnt. Gefordert wird, dass das Detektionsergebnis unabhängig von weiterer User-Interaktion bleibt und keine dialogische Auseinandersetzung mit der Anwendung möglich ist (TN_1, Zeile 120 - 122). Diese Forderung basiert auf der Erfahrung, dass Chatbots durch Nutzende beeinflussbar sind und dadurch das Ergebnis der Detektion verändert werden könnte. Die geforderte Unabhängigkeit bezieht sich weiterhin auch auf die zugrunde liegenden Daten der KI. Es soll vermieden werden, dass personenbezogene Informationen des Users in die Bewertung einfließen, etwa zur Anpassung an Sprachstil oder politische Haltung (TN_2, Zeile 56). Die Anwendung soll als neutrale Instanz konzipiert sein, unabhängig auch von möglichen Interessen staatlicher oder unternehmerischer Akteure (TN_2, Zeile 57–59). Folglich wird gefordert, dass die Anwendung frei von parteipolitischen Einflüssen agiert und ausschließlich auf Grundlage neutraler, objektiver Kriterien Inhalte auf Misinformation prüft und bewertet. Diese Bewertung soll visuell-textlich für alle Nutzenden schnell und verständlich erfassbar sein. Es soll die Möglichkeit geben, die Quellen und Gründe für die Bewertung einzusehen und eigenständig zu überprüfen.

### Quantitative Ergebnisse

#### Stichprobe

Der Datensatz für die Analyse wurde vor einem Team vorgefiltert, eine zusätzliche Filterung oder Ausschlusskriterien nach Datenerhebung war nicht erforderlich. Es wurden *N* = 176 Teilnehmenden in die folgenden Analyse berrücksichtigt Die Stichprobe verteilte sich nahezu gleichmäßig: 89 Personen bearbeiteten die Aufgabe mit einer empfehlenden KI-Variante, während 87 Teilnehmende mit einer evaluativen Variante arbeiteten. Alle Teilnehmenden durchliefen eine Phase ohne und eine Phase mit KI-Unterstützung. Das Alter wurde metrisch erhoben. Das Durchschnittsalter betrug *M* = 25,9 (*Mdn* = 22, *SD* = 10,6). Eine obere Altersgrenze wurde nicht gesetzt, um eine möglichst diverse Stichprobe zu gewährleisten. So liegt das Alter der Teilnehmenden zwischen 18 und 69 Jahren. Das Geschlecht wurde nominalskaliert erfasst (männlich, weiblich, divers). 90 Teilnehmende identifizierten sich als weiblich, 84 als männlich und zwei als non-binär. Auf die Frage nach der Vorerfahrung im Umgang mit Künstlicher Intelligenz (metrisch) stuften neun Teilnehmende diese als sehr hoch ein, 47 als eher hoch, während zehn Personen angaben, über sehr geringe Vorerfahrung zu verfügen und 25 über eher geringe Vorerfahrung. Die Mehrheit von 85 Teilnehmenden bewertete ihre Vorerfahrung als mittelmäßig.

Die Technikaffinität der Stichprobe wurde anhand der ATI-Skala (Fragebogen zur Affinität gegenüber Technikinteraktion, @franke2019ati) erhoben. Die interne Konsistenz der Skala ergab einen Wert für Cronbach’s Alpha von α = 0.91, was auf eine sehr hohe interne Konsistenz der Skala hinweist. Die Itemanalyse ergab, dass fast alle Items zu einer hohen Reliabilität beitrugen. Nur das Item ati_3 zeigte mit eine vergleichsweise geringe Trennschärfe, doch würde der Alpha-Wert bei Entfernung dieses Items nur geringfügig auf 0.92 steigen. Da der Gesamtwert jedoch bereits sehr hoch ist und das Item theoretisch relevant erscheint, wurde entschieden, ati_3 ("In erster Linie beschäftige ich mich mit technischen Systemen, weil ich muss.") in der Skala zu belassen. Der durchschnittliche Skalenwert lag bei *M* = 3.7 (*SD* = 0.58), was auf eine insgesamt moderate Technikaffinität der Teilnehmenden hinweist.

#### Deskriptive Statistik

Zur quantitativen Auswertung der Studie wurden zwei abhängige Variablen betrachtet: die subjektive mentale Belastung sowie das Vertrauen in das verwendete KI-System. Die mentale Belastung wurde mithilfe der DLR-Workload-Skala (@dlr119443) nach der Nutzung der jeweiligen KI-Anwendung erfasst. Die Erhebung erfolgte über eine visuelle Analogskala im Bereich von 0 bis 200 Punkten. Das Vertrauen in das System wurde anhand eines Einzelitems auf einer 5-Punkte-Likert-Skala gemessen (1 = sehr geringes Vertrauen, 5 = sehr hohes Vertrauen) nach @Mandsen_Gregor.

@tbl-belastung zeigt die deskriptiven Statistiken beider Variablen. Die mentale Belastung weist bei *N* = 176 einen Mittelwert von *M* = 105.90 (SD = 22.63) auf, bei einem Wertebereich von 44.38 bis 178.13. Dies deutet auf eine mittlere bis leicht erhöhte Beanspruchung hin. Das Vertrauen in das KI-System lag im Mittel bei *M* = 3.35 (*SD* = 0.93) und spiegelt somit eine eher neutrale bis tendenziell positive Haltung gegenüber der KI-Anwendung wider. Zur differenzierten Betrachtung der mentalen Belastung nach Interaktionstyp wurde zusätzlich eine Gruppierung nach KI-Typ vorgenommen (evaluativ vs. empfehlend). Dies wurde in @tbl-belastung zusammengefasst.

```{r}
#| label: setup
#| echo: false
#| message: false
#| warning: false

# Pakete laden
library(dplyr)
library(tidyr)
library(psych)
library(knitr)
library(kableExtra)
library(ggplot2)


# Daten einlesen
data <- read.csv("data_combined.csv", stringsAsFactors = FALSE)

# Items zur Belastung nach KI-Anwendung
ki_items <- grep("^ki_dlr_wat_", names(data), value = TRUE)

# Mittelwert berechnen (Belastung nach KI-Nutzung)
data$DLR_KI <- rowMeans(data[, ki_items], na.rm = TRUE)

# Gruppierungsvariable erstellen anhand vorhandener Werte
data$group <- NA
data$group[!is.na(data$workload_mean_E)] <- "Evaluativ"
data$group[!is.na(data$workload_mean_R)] <- "Empfehlend"

# Falls 'group' NA ist, und DLR_KI-Wert vorhanden, trotzdem mitnehmen
data_grouped <- data %>%
  filter(!is.na(group) & !is.na(DLR_KI))

# Deskriptive Statistik nach Gruppen
desc_stats_grouped <- data_grouped %>%
  group_by(group) %>%
  summarise(
    Stichprobenumfang = sum(!is.na(DLR_KI)),
    Mittelwert = mean(DLR_KI, na.rm = TRUE),
    Median = median(DLR_KI, na.rm = TRUE),
    Standardabweichung = sd(DLR_KI, na.rm = TRUE),
    Minimum = min(DLR_KI, na.rm = TRUE),
    Maximum = max(DLR_KI, na.rm = TRUE),
    .groups = "drop"
  )

#| label: tbl-belastung
#| tbl-cap: "Deskriptive Statistik der mentalen Belastung"
# Tabelle anzeigen
kable(desc_stats_grouped,
      caption = "Deskriptive Statistik der mentalen Belastung nach KI-Typ",
      booktabs = TRUE, digits = 2) %>%
  kable_styling()

```

```{r}
#| echo: false
#| message: false
#| warning: false

# Pakete laden
library(psych)
library(knitr)

# Daten laden
data <- read.csv("data_combined.csv", stringsAsFactors = FALSE)

# Items für die KI-Skala
ki_items <- grep("^ki_dlr_wat_", names(data), value = TRUE)
data$DLR_KI <- rowMeans(data[, ki_items], na.rm = TRUE)

# Vertrauen (invertiert)
data$Trust <- 7 - as.numeric(data$aiInformationDistrust)

# Cronbach's Alpha für mentale Belastung (KI)
alpha_ki <- psych::alpha(data[, ki_items], check.keys = TRUE)$total$raw_alpha


# Zusammenfassung 
summary_table <- data.frame(
  Variable = c("Mentale Belastung", "Vertrauen"),
  Stichprobenumfang = c(sum(!is.na(data$DLR_KI)), sum(!is.na(data$Trust))),
  Mittelwert = c(mean(data$DLR_KI, na.rm = TRUE), mean(data$Trust, na.rm = TRUE)),
  Median = c(median(data$DLR_KI, na.rm = TRUE), median(data$Trust, na.rm = TRUE)),
  Standardabweichung = c(sd(data$DLR_KI, na.rm = TRUE), sd(data$Trust, na.rm = TRUE)),
  Minimum = c(min(data$DLR_KI, na.rm = TRUE), min(data$Trust, na.rm = TRUE)),
  Maximum = c(max(data$DLR_KI, na.rm = TRUE), max(data$Trust, na.rm = TRUE)),
  Cronbachs_Alpha  = c(round(alpha_ki, 3),"-")
)

#| label: tbl-belastung-vertrauen
#| tbl-cap: "Deskriptive Statistik für mentale Belastung und Vertrauen"

# Tabelle 
kable(summary_table, caption = "Deskriptive Statistik für mentale Belastung und Vertrauen", digits = 3)

```

#### Inferenzstatistik

Die durchschnittliche mentale Belastung unterschied sich nicht signifikant zwischen Teilnehmenden in der Bedingung „Empfehlend“ (*M* = 104.85, *SD* = 20.31) und der Bedingung „Evaluativ“ (*M* = 106.97, *SD* = 20.87), *t*(173.63) = -0,62, *p* = .536 und *d* = 0.09. Die Effektstärke entspricht nach Cohen (1988) keinem Effekt. Dies deutet darauf hin, dass es keinen bedeutsamen Unterschied in der empfundenen mentalen Belastung zwischen den beiden KI-Typen gibt. Dies ist im Dot-and-Whisker-Plot @fig-belastung-plot veranschaulicht.

```{r}
#| echo: false
#| message: false
#| warning: false


# Unabhängiger t-Test (Welch-Test) für mentale Belastung
t_test_result <- t.test(DLR_KI ~ group,
                        data = data_grouped,
                        var.equal = FALSE)

# Daten für Plot vorbereiten
plot_data <- data_grouped %>%
  group_by(group) %>%
  summarise(
    mean = mean(DLR_KI, na.rm = TRUE),
    sd = sd(DLR_KI, na.rm = TRUE),
    n = n(),
    se = sd / sqrt(n),
    ci_lower = mean - qt(0.975, df = n - 1) * se,
    ci_upper = mean + qt(0.975, df = n - 1) * se,
    .groups = "drop"
  )

#| label: fig-belastung-plot
#| fig-cap: "Mentale Belastung nach KI-Typ"

# Dot-and-Whisker-Plot 
ggplot(plot_data, aes(x = group, y = mean)) +
  geom_point(size = 4, color = "#2C3E50") +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.1, color = "#2C3E50") + scale_y_continuous(limits = c(95, 115)) +
  labs(
    title = "Abbildung: Mentale Belastung nach KI-Typ",
    x = "KI-Typ",
    y = "Mentale Belastung (0–200)"
  ) +
  theme_minimal(base_size = 14)

#| echo: false
#| message: false
#| warning: false

```

Zudem wurde eine Korrelationsanalyse nach Pearson durchgeführt, um den Zusammenhang zwischen der subjektiv empfundenen mentalen Belastung und dem Vertrauen in das KI-System zu untersuchen. Die Analyse ergab keinen signifikanten Zusammenhang (*r*(174) = .089, *p* = .238). Dieser Wert für Pearson’s r entspricht laut @bourier_zusammenhang_2025 keinem Effekt. Es zeigt sich zwar eine tendenziell positive Beziehung, jedoch ohne ausreichende statistische Evidenz. Entsprechend kann nicht belegt werden, dass eine höhere empfundene kognitive Belastung mit einem geringeren Vertrauen in das System einhergeht. Diese Beziehung wird in @fig-korrelation-belastung-vertrauen, dem Scatter-Plot mit Regressionslinie, grafisch dargestellt. Die Punktwolke zeigt eine leichte positive Tendenz, unterstützt durch die eingefügte Regressionslinie.

```{r}
#| echo: false
#| message: false
#| warning: false

#| label: fig-korrelation-belastung-vertrauen
#| fig-cap: "Zusammenhang: mentaler Belastung und Vertrauen"
# Scatter-Plot
ggplot(data, aes(x = DLR_KI, y = Trust)) +
  geom_jitter(width = 0.2, height = 0.2, color = "#0072B2", alpha = 0.6, size = 2) +
  geom_smooth(method = "lm", se = FALSE, color = "#D55E00") +
  labs(
    title = "Zusammenhang: mentale Belastung (DLR) und Vertrauen",
    x = "Mentale Belastung (DLR-Skala, mit KI)",
    y = "Vertrauen in das KI-System"
  ) +
  theme_minimal(base_size = 14)

# Korrelations-Test
cor_test <- cor.test(data$DLR_KI, data$Trust, method = "pearson")
```

Zusammenfassend lässt sich festhalten, dass sich die beiden getesteten KI-Systeme in ihrer Auswirkung auf die mentale Belastung nicht signifikant voneinander unterschieden. Beide Varianten führten zu vergleichbaren Belastungswerten, was darauf hinweist, dass der eingesetzte KI-Typ – evaluativ oder empfehlend – keinen entscheidenden Einfluss auf das subjektive Belastungsempfinden hatte. Zudem ergab die Korrelationsanalyse zwischen der empfundenen mentalen Belastung und dem Vertrauen in das System keinen signifikanten Zusammenhang. Teilnehmende, die das System als stärker belastend wahrnahmen, berichteten zwar tendenziell ein geringeres Vertrauen in die KI, diese Beziehung war jedoch statistisch nicht belastbar. Diese Ergebnisse sind insbesondere für die nutzerzentrierte Gestaltung von KI-Anwendungen bedeutsam, da sie nahelegen, dass eine wahrgenommene kognitive Entlastung potenziell das Vertrauen der Nutzenden in ein KI-System fördern kann, auch wenn dies in der vorliegenden Stichprobe nicht eindeutig bestätigt wurde.

## Diskussion

-   Beantwortung der Forschungsfrage(n).

<!-- -->

-   Wissenschaftliche/gesellschaftliche Implikationen.

-   Limitationen der Studie.

-   Zukünftige Arbeit.

    💡 Der Abschnitt „Diskussion“ dient der Interpretation der Ergebnisse.

Was bedeuten die Ergebnisse in Bezug auf die Forschungsfrage(n)?

-   kurze Zusammenfassung der Studie, jeweils ein bis zwei Sätze zu (Ziel, Methode, Ergebnisse) (ausführlicher, sodass auch verstanden werden kann, wenn der Rest des extended Abstracts nicht gelesen wurde)

-   explizite Antwort der Forschungsfrage (super für Rückgriff)

Was lässt sich aus der Studie schlussfolgern (Wissenschaft/Gesellschaft)?

-   Auwirkung auf Wissenschaft (Widersprüche (gründe?), Erweiterung, Nutzungsmöglichkeit)

-   erklären Ergebnisse Phänomene/menschliches Verhalten in bestimmten Situationen?

-   unterstützend für Implementierung von Gesetzen/Richtlinien?

-   z.B. Ergebnisse legen nahe, dass bestimmte Features Mehrwert für Weiterentwicklung von KI-Tools zur Detektion von Misinformation...

Was sind die Einschränkungen der Studie?

-   Zu beachten ist, dass die Stichprobe nicht repräsentativ war, da an einer Universität über einen E-Mail-Verteiler rekrutiert worden ist. Insofern handelt es sich womöglich um eine selbst-selektierte Stichprobe, die mehr Interesse an Nachhaltigkeit als die allgemeine Population in Deutschland ... siehe Studierende ähnlicher Kohorte, Technik schwerpunkt, nicht repäsentativ für allgemeine Population (dazu Websiten unspezifisch, einbindung in Kontext noch mal speziellere Anforderungen, hier soweit Grundlagenuntersuchung, nicht konkreter Kontext)

-   Beschränkung hier Modell textliche Misinformation, nicht jede Form textuell;

-   Zeitpunkt, politisch Misinformation und großer Boom wie Skepsis KI?

Was sind, auf Basis der Implikationen und Limitationen, sinnvolle Folgestudien?

## Code of Conduct {.unnumbered}

Als Arbeitsgruppe verpflichten wir uns einem gemeinsamen Verhaltenskodex, der den Anforderungen wisschenschaftlicher Forschung und respektvollen Umgang entspricht. Dieser bildet die Grundlage für die Zusammenarbeit und beinhaltet folgende Punkte:

Wir begegenen einander offen und respektvoll. Konstruktives Feedback wird wertgeschätzend geäußert, aufgenommen und besprochen. Dabei sind unterschiedliche Perspektiven und Meinungen willkommen und werden als zur Zusammenarbeit gehörend betrachtet. Entsprechend wird sich im Konfliktfall gemeinsam bemüht wird, eine zufriedenstellende Lösung zu finden. Die Mitglieder werden als gleichberechtig angesehen und behandelt.

Es wird eine ausgewogene Aufteilung der Arbeitslast angestrebt, die die Stärken und Schwächen der einzelnen Mitgleider berücksichtigt. Folglich verpflichten wir uns zu einer offenen und direkten Kommunikation. Im Falle möglicher Verzögerungen, Probleme oder unerwarteter Auslastungen wird frühzeitig kommuniziert und sich gegenseitig unterstützt.

Wir verpflichten uns der Einhaltung wissenschaftlicher Standards, mit besonderer Umsicht gegenüber erhobener, personenbezogener Daten und der Wahrung ihrer Vertraulichkeit. Ebenso wird streng auf sorgfältigen Recherche und korrekte Zitation geachtet. Die verantwortungsvolle Nutzung von AI Tools erfolgt transparent mit Beschreibung und Kennzeichnung der Nutzung. So wurde @openai2025chatgpt genutzt, um die Formulierungen zu präzisieren, Code zu überprüfen und die Rechtschreibung zu kontrollieren.

## Literaturverzeichnis {.unnumbered}

## Anhang 1 - Rekrutierungstext {.unnumbered}

**Online-Studie zur Erkennung von Falschinformationen – Teilnehmende gesucht!**

Wir führen im Rahmen eines Forschungsmoduls eine **Online-Umfrage** durch (Dauer: ca. **60 Minuten**). In der Studie wird untersuchen, wie verschiedene KI-gestützte Systeme Menschen dabei unterstützen können, Falschinformationen zu erkennen.

G**esucht werden Teilnehmende, die...**

\- mindestens **18 Jahre alt** sind,

\- **nicht** an der Veranstaltung **SMNF** teilnehmen,

\- **Social Media zumindest gelegentlich nutzen**,

\- **Deutsch** sprechen und verstehen,

\- Zugriff auf ein **Tablet, Laptop oder PC** haben,

\- einen **modernen Browser** nutzen (z.B. Chrome, Firefox oder Safari).

**Wichtig:** Alle erhobenen Daten sind **vollständig anonym** und **nicht auf Sie rückführbar**.

Interesse? Hier geht's direkt zur Studie:

https://dsslab.hciuse.sh/study/pilot?groupId=gr-a2

Danke für Ihre Unterstützung!
